# -*- coding: utf-8 -*-
"""audioTranslator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ntNOD2VCVDTZbuQHHlxl-5N9hc0-h835
"""

# 1) Instalar dependências
!pip install SpeechRecognition pydub transformers sentencepiece torch

# 2) Imports
import os
import speech_recognition as sr
from pydub import AudioSegment
from google.colab import files
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# 3) Função para conversão de áudio
def convert_to_wav(input_file):
    filename, ext = os.path.splitext(input_file)
    if ext.lower() != ".wav":
        sound = AudioSegment.from_file(input_file)
        wav_filename = filename + ".wav"
        sound.export(wav_filename, format="wav")
        return wav_filename
    return input_file

# 4) Upload do áudio
print("📥 Faça o upload do seu áudio em inglês:")
uploaded = files.upload()
audio_file = list(uploaded.keys())[0]
print(f"🔊 Arquivo recebido: {audio_file}")

# 5) Conversão se necessário
audio_wav = convert_to_wav(audio_file)

# 6) Reconhecimento de fala
r = sr.Recognizer()
with sr.AudioFile(audio_wav) as source:
    audio_data = r.record(source)

try:
    print("📝 Transcrevendo áudio...")
    texto_en = r.recognize_google(audio_data, language="en-US")
    print(f"\n🇬🇧 Texto em inglês:\n{texto_en}")
except sr.UnknownValueError:
    print("❌ Não foi possível entender o áudio.")
    raise SystemExit
except sr.RequestError as e:
    print(f"❌ Erro na API de voz: {e}")
    raise SystemExit

# 7) Tradução com NLLB (Meta)
print("\n🌐 Traduzindo para português com NLLB...")
model_name = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Definir os tokens de idioma
src_lang = "eng_Latn"
tgt_lang = "por_Latn"

# Preparar entrada
inputs = tokenizer(texto_en, return_tensors="pt", padding=True)
inputs["forced_bos_token_id"] = tokenizer.convert_tokens_to_ids(tgt_lang)

# Gerar tradução
translated_tokens = model.generate(**inputs, max_length=1000)
translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]

print(f"\n🇧🇷 Tradução:\n{translated_text}")